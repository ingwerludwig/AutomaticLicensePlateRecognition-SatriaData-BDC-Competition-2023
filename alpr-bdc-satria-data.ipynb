{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "XESU09fcRJEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easyocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz8X8xIItbzs",
        "outputId": "c42bd146-e47b-472e-870e-01717ad7265a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/2.9 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.15.2+cu118)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.7.0.72)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (8.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.3)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.1)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (813 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.9/813.9 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5->easyocr) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->easyocr) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->easyocr) (16.0.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi->easyocr) (1.16.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (23.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5->easyocr) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
            "Installing collected packages: pyclipper, ninja, python-bidi, easyocr\n",
            "Successfully installed easyocr-1.7.0 ninja-1.11.1 pyclipper-1.3.0.post4 python-bidi-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import easyocr"
      ],
      "metadata": {
        "id": "LsIa2T2EYdBk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Prepare the Dataset\n",
        "train_csv_file = 'DataTrain.csv'\n",
        "test_csv_file = 'DataTest.csv'\n",
        "train_image_folder = os.path.join(os.getcwd(), 'DataTrain')\n",
        "test_image_folder = os.path.join(os.getcwd(), 'DataTest')\n",
        "\n",
        "# Step 2: Load and Preprocess the Dataset\n",
        "current_directory = os.getcwd()\n",
        "train_csv_path = os.path.join(current_directory, train_csv_file)\n",
        "test_csv_path = os.path.join(current_directory, test_csv_file)\n",
        "train_image_folder_path = os.path.join(current_directory, train_image_folder)\n",
        "test_image_folder_path = os.path.join(current_directory, test_image_folder)\n",
        "\n",
        "df_train = pd.read_csv(train_csv_path)\n",
        "df_test = pd.read_csv(test_csv_path)\n",
        "\n",
        "train_image_paths = df_train['NameofFile'].apply(lambda x: os.path.join(train_image_folder_path, x)).values\n",
        "train_labels = df_train['Vehicleregistrationplate'].values\n",
        "\n",
        "test_image_paths = df_test['Name of File'].apply(lambda x: os.path.join(test_image_folder_path, x)).values\n",
        "\n",
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "\n",
        "# Preprocess the images (e.g., resize, normalize, convert to tensors)\n",
        "image_size = (224, 224)\n",
        "\n",
        "train_images = []\n",
        "for image_path in train_image_paths:\n",
        "    image = load_img(image_path, target_size=image_size)\n",
        "    image = img_to_array(image)\n",
        "    image = image / 255.0  # Normalize the image\n",
        "    train_images.append(image)\n",
        "train_images = np.array(train_images)\n",
        "\n",
        "# Calculate the number of plate classes\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Step 3: Perform Character Segmentation\n",
        "def perform_character_segmentation(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "    # Convert image to 8-bit unsigned integer\n",
        "    binary = np.uint8(binary)\n",
        "\n",
        "    # Apply connected component analysis\n",
        "    connectivity = 8\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary, connectivity, cv2.CV_32S)\n",
        "\n",
        "    # Extract individual character images\n",
        "    segmented_characters = []\n",
        "    for i in range(1, num_labels):\n",
        "        x, y, w, h, area = stats[i]\n",
        "        if area > 100:  # Filter out small components\n",
        "            character = image[y:y+h, x:x+w]\n",
        "            segmented_characters.append(character)\n",
        "\n",
        "    return segmented_characters\n",
        "\n",
        "\n",
        "\n",
        "# Step 4: Build the Model Architecture\n",
        "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "\n",
        "train_characters = []\n",
        "train_labels_extended = []\n",
        "for i in range(len(train_images)):\n",
        "    image = train_images[i]\n",
        "    characters = perform_character_segmentation(image)\n",
        "    train_characters.extend(characters)\n",
        "    label = train_labels[i]\n",
        "    train_labels_extended.extend([label] * len(characters))\n",
        "\n",
        "train_characters = np.array(train_characters)\n",
        "train_labels_extended = np.array(train_labels_extended)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_characters, train_labels_extended, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# Step 6: Evaluate and Fine-Tune\n",
        "\n",
        "# Step 7: Test the Model\n",
        "# Make predictions on test images\n",
        "test_characters = []\n",
        "for image_path in test_image_paths:\n",
        "    image = load_img(image_path, target_size=image_size)\n",
        "    image = img_to_array(image)\n",
        "    image = image / 255.0  # Normalize the image\n",
        "    characters = perform_character_segmentation(image)\n",
        "    test_characters.extend(characters)\n",
        "\n",
        "test_characters = np.array(test_characters)\n",
        "predictions = model.predict(test_characters)\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "# Print predicted labels\n",
        "for image_path, predicted_label in zip(test_image_paths, predicted_labels):\n",
        "    print(f\"Image: {image_path} - Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJB2iF6O8mw9",
        "outputId": "6f46f04e-fce8-46e6-af6c-56d540a1ba53"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "25/25 [==============================] - 737s 28s/step - loss: 6.5216 - accuracy: 0.0100\n",
            "Epoch 2/15\n",
            "25/25 [==============================] - 701s 28s/step - loss: 6.2492 - accuracy: 0.0200\n",
            "Epoch 3/15\n",
            "25/25 [==============================] - 700s 28s/step - loss: 6.0475 - accuracy: 0.0162\n",
            "Epoch 4/15\n",
            "25/25 [==============================] - 663s 26s/step - loss: 5.7888 - accuracy: 0.0275\n",
            "Epoch 5/15\n",
            "25/25 [==============================] - 650s 26s/step - loss: 5.3771 - accuracy: 0.0338\n",
            "Epoch 6/15\n",
            "25/25 [==============================] - 644s 26s/step - loss: 4.9201 - accuracy: 0.0600\n",
            "Epoch 7/15\n",
            "25/25 [==============================] - 639s 26s/step - loss: 4.5468 - accuracy: 0.0887\n",
            "Epoch 8/15\n",
            "25/25 [==============================] - 646s 26s/step - loss: 4.1109 - accuracy: 0.1163\n",
            "Epoch 9/15\n",
            "25/25 [==============================] - 646s 26s/step - loss: 3.6126 - accuracy: 0.1800\n",
            "Epoch 10/15\n",
            "25/25 [==============================] - 630s 25s/step - loss: 3.2473 - accuracy: 0.2325\n",
            "Epoch 11/15\n",
            "25/25 [==============================] - 629s 25s/step - loss: 2.7848 - accuracy: 0.2950\n",
            "Epoch 12/15\n",
            "25/25 [==============================] - 632s 25s/step - loss: 2.3330 - accuracy: 0.3837\n",
            "Epoch 13/15\n",
            "25/25 [==============================] - 635s 25s/step - loss: 1.8947 - accuracy: 0.4950\n",
            "Epoch 14/15\n",
            "25/25 [==============================] - 634s 25s/step - loss: 1.5576 - accuracy: 0.5763\n",
            "Epoch 15/15\n",
            "25/25 [==============================] - 636s 25s/step - loss: 1.3228 - accuracy: 0.6237\n",
            "4/4 [==============================] - 19s 4s/step\n",
            "Image: /content/DataTest/DataTest1.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest2.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest3.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest4.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest5.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest6.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest7.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest8.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest9.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest10.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest11.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest12.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest13.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest14.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest15.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest16.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest17.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest18.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest19.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest20.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest21.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest22.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest23.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest24.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest25.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest26.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest27.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest28.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest29.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest30.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest31.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest32.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest33.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest34.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest35.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest36.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest37.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest38.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest39.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest40.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest41.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest42.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest43.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest44.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest45.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest46.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest47.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest48.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest49.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest50.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest51.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest52.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest53.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest54.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest55.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest56.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest57.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest58.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest59.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest60.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest61.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest62.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest63.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest64.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest65.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest66.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest67.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest68.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest69.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest70.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest71.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest72.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest73.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest74.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest75.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest76.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest77.png - Predicted Label: AE7296XA\n",
            "Image: /content/DataTest/DataTest78.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest79.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest80.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest81.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest82.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest83.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest84.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest85.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest86.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest87.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest88.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest89.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest90.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest91.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest92.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest93.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest94.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest95.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest96.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest97.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest98.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest99.png - Predicted Label: B1739TZI\n",
            "Image: /content/DataTest/DataTest100.png - Predicted Label: B1739TZI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKxuVSyA-zgb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}